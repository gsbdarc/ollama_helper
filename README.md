# Ollama on Stanford Computing Clusters
A hands-on guide and accompanying scripts for running Ollama (local LLM inference) on Stanford’s Yen, Sherlock, and Marlowe clusters.

Based on the blog post “[Running Ollama on Stanford Computing Clusters](link-to-your-blog)” by the GSB DARC Team. 

## Overview

This repository contains:

- **`ollama.sh`**: helper function to launch and manage the Ollama server.  
- **`test.py`**: example Python script to verify your Ollama server.  

Follow the steps below to get up and running.

## Prerequisites

- Access to Stanford HPC clusters (Yen, Sherlock, or Marlowe)  

## Installation

```bash title="Clone This Code Repo"
# 1. Clone this repo into your project space
cd </your/project/path>
git clone https://github.com/gsbdarc/ollama.git
cd ollama
```
